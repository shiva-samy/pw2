huggingface-hub==0.23.5
llama-index==0.11.17
llama-index-llms-huggingface==0.3.5
llama-index-embeddings-huggingface==0.3.1
llama-index-embeddings-huggingface-api==0.2.1
llama-index-llms-huggingface-api==0.2.0
python-dotenv

from datetime import datetime
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_parse import LlamaParse
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI
from huggingface_hub import snapshot_download
import os, time
from dotenv import load_dotenv
import gradio as gr
from pathlib import Path
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

# Load environment variables
load_dotenv()

# Paths for storing downloaded models and reports
BASE_DIR = Path("models")
REPORTS_DIR = Path("reports")
LLM_MODEL_PATH = BASE_DIR / "llms"
EMBED_MODEL_PATH = BASE_DIR / "embeddings"

# Ensure the base directories exist
BASE_DIR.mkdir(parents=True, exist_ok=True)
REPORTS_DIR.mkdir(parents=True, exist_ok=True)

# Models to be used
LLM_MODEL_NAME = "mistralai/Mixtral-8x7B-Instruct-v0.1"
EMBED_MODEL_NAME = "BAAI/bge-large-en"

# Qdrant settings
QDRANT_HOST = os.getenv("QDRANT_HOST")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
QDRANT_COLLECTION_NAME = "pw2-vdb"

def download_model(model_name, destination):
    if not destination.exists():
        print(f"Downloading {model_name} to {destination}...")
        snapshot_download(repo_id=model_name, local_dir=destination)
        print(f"Model {model_name} downloaded to {destination}.")
    else:
        print(f"Model {model_name} already exists at {destination}.")

download_model(EMBED_MODEL_NAME, EMBED_MODEL_PATH)

# Initialize Qdrant client
qdrant_client = QdrantClient(url=QDRANT_HOST, api_key=QDRANT_API_KEY)

# Create or load Qdrant collection
try:
    qdrant_client.get_collection(QDRANT_COLLECTION_NAME)
except:
    qdrant_client.create_collection(
        collection_name=QDRANT_COLLECTION_NAME,
        vectors_config=VectorParams(size=1024, distance=Distance.COSINE)  # Adjust size based on your embedding model
    )
    print("Database didn't exist, so created pw2-vdb!")

# Global variables
vector_index = None
parser = LlamaParse(api_key=os.getenv("LLAMA_INDEX_API"), result_type='markdown')
file_extractor = {'.pdf': parser, '.docx': parser, '.doc': parser, '.txt': parser, '.csv': parser, '.xlsx': parser, '.pptx': parser, '.html': parser, '.jpg': parser, '.jpeg': parser, '.png': parser, '.webp': parser, '.svg': parser}

def load_files(file_paths):
    try:
        global vector_index
        documents = []
        for file_path in file_paths:
            start_time = time.time()
            document = SimpleDirectoryReader(input_files=[file_path], file_extractor=file_extractor).load_data()
            documents.extend(document)
            print(f"Parsing done for {file_path} in {time.time() - start_time} seconds")

        embed_model = HuggingFaceEmbedding(model_name=str(EMBED_MODEL_PATH))
        start_time = time.time()
        embeddings = embed_model.get_text_embedding_batch([doc.text for doc in documents])
        print(f"Embedding generation time: {time.time() - start_time} seconds")

        if vector_index is None:
            vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)
        else:
            for doc in documents:
                vector_index.insert(doc)
        print("All documents processed and indexed.")

        # Store embeddings in Qdrant
        start_time = time.time()
        for doc, embedding in zip(documents, embeddings):
            qdrant_client.upsert(
                collection_name=QDRANT_COLLECTION_NAME,
                points=[
                    {
                        "id": doc.doc_id,
                        "vector": embedding,
                        "payload": {"text": doc.text, "metadata": doc.metadata}
                    }
                ]
            )
        print(f"Qdrant upsert time: {time.time() - start_time} seconds")

        filenames = [os.path.basename(file_path) for file_path in file_paths]
        return f"Ready to chat about: {', '.join(filenames)}"
    except Exception as e:
        return f"An error occurred: {e}"

def respond(message, history):
    try:
        if vector_index is None:
            return history + [(message, "Please upload documents first.")]

        # Initialize the LLM
        llm = HuggingFaceInferenceAPI(model_name=LLM_MODEL_NAME, token=os.getenv("TOKEN"))

        # Query the combined VectorStoreIndex
        query_engine = vector_index.as_query_engine(llm=llm)
        bot_message = query_engine.query(message)

        # Check if the response is relevant to the documents
        if "I don't know" in str(bot_message) or "not found" in str(bot_message).lower():
            bot_message = "I don't have information about that in the uploaded documents."

        print(f"\n{datetime.now()}:{LLM_MODEL_NAME}:: {message} --> {str(bot_message)}\n")

        # Append the user's message and bot's response to the history
        history.append((message, str(bot_message)))

        # Return the updated history
        return history
    except Exception as e:
        error_message = "Please upload documents first." if "'NoneType' object has no attribute 'as_query_engine'" in str(e) else f"An error occurred: {e}"
        history.append((message, error_message))
        return history

def generate_markdown_report(report_content):
    """Generate a Markdown report."""
    # Create the report content
    markdown_content = f"# Generated Report\n\n{report_content}"

    # Save the Markdown file
    report_filename = REPORTS_DIR / f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    with open(report_filename, "w", encoding="utf-8") as f:
        f.write(markdown_content)

    return f"Report generated and saved at {report_filename}"

def generate_report(prompt):
    try:
        if vector_index is None:
            return "Please upload documents first."

        # Initialize the LLM and query engine
        llm = HuggingFaceInferenceAPI(model_name=LLM_MODEL_NAME, token=os.getenv("TOKEN"))
        query_engine = vector_index.as_query_engine(llm=llm)

        # Generate the report structure based on the user's prompt
        report_structure = llm.complete(
            f"Generate a report structure about Shiva Aravindha Samy A's education, skills, projects, and achievements based on the following prompt: {prompt}"
        ).text
        print(f"Report Structure: {report_structure}")

        # Generate content for each section in the report structure
        report_content = ""
        for section in report_structure.split("\n"):
            if section.strip():
                # Query the document for relevant information
                query = f"Provide detailed information about Shiva Aravindha Samy A's {section.lower()} based on the uploaded document."
                answer = str(query_engine.query(query))

                # Append the section and its content to the report
                report_content += f"# {section}\n\n{answer}\n\n"

        # Generate the Markdown report
        return generate_markdown_report(report_content)
    except Exception as e:
        return f"Error generating report: {e}"

def main_interface():
    with gr.Blocks() as demo:
        gr.Markdown("# DocBotðŸ“„ðŸ¤– - Document Analysis Tool")
        with gr.Row():
            with gr.Column(scale=1):
                file_input = gr.File(file_count="multiple", type='filepath', label="Upload document(s)")
                btn = gr.Button("Submit", variant='primary', interactive=False)
                output = gr.Text(label='Status')
                chat_button = gr.Button("Open Chat")
                report_button = gr.Button("Open Report Generator")
        
        # Define chat and report interfaces
        with gr.Column(visible=False) as chat_interface:
            chatbot = gr.Chatbot(height=500)
            textbox = gr.Textbox(placeholder="Ask me anything about the uploaded documents!", container=False)
            chat_interface_visible = gr.Checkbox(value=False, visible=False)
        
        with gr.Column(visible=False) as report_interface:
            report_prompt = gr.Textbox(label="Enter the report structure (e.g., 'Generate a report with sections for personal information, education, and skills')")
            generate_button = gr.Button("Generate Report")
            report_output = gr.Textbox(label="Generated Report", interactive=False)
            report_interface_visible = gr.Checkbox(value=False, visible=False)
        
        # Enable Submit button when files are selected
        def enable_submit(file_paths):
            return gr.update(interactive=bool(file_paths))

        file_input.change(fn=enable_submit, inputs=[file_input], outputs=[btn])

        # Button actions
        btn.click(fn=load_files, inputs=[file_input], outputs=output)
        
        def toggle_chat_interface():
            return gr.update(visible=True), gr.update(visible=False)
        
        def toggle_report_interface():
            return gr.update(visible=False), gr.update(visible=True)
        
        chat_button.click(fn=toggle_chat_interface, outputs=[chat_interface, report_interface])
        report_button.click(fn=toggle_report_interface, outputs=[chat_interface, report_interface])
        
        # Chat interface
        textbox.submit(fn=respond, inputs=[textbox, chatbot], outputs=[chatbot])
        
        # Report interface
        generate_button.click(fn=generate_report, inputs=[report_prompt], outputs=[report_output])
    
    demo.launch()

if __name__ == "__main__":
    main_interface()